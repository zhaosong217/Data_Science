{"cells":[{"cell_type":"markdown","id":"05753982-a90f-4545-ab54-41440bb255a7","metadata":{},"outputs":[],"source":["<p style=\"text-align:center\">\n","    <a href=\"https://skills.network/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkML0101ENSkillsNetwork20718538-2022-01-01\" target=\"_blank\">\n","    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/assets/logos/SN_web_lightmode.png\" width=\"200\" alt=\"Skills Network Logo\">\n","    </a>\n","</p>\n","\n","# Lab: K-Nearest Neighbors Classifier\n","\n","Estimated time needed: **25** minutes\n","\n","## Objectives\n","\n","After completing this lab you will be able to:\n","\n","*   Use K-Nearest neighbors to classify data\n","*   Apply KNN classifier on a real world data set \n"]},{"cell_type":"markdown","id":"ae0f5505-0f12-4027-9be8-526557811415","metadata":{},"outputs":[],"source":["In this lab you will load a customer data set, fit the data, and use K-Nearest Neighbors to predict a data point.\n"]},{"cell_type":"markdown","id":"a95caace-1aa9-40c9-859b-0b5ca4937fd8","metadata":{},"outputs":[],"source":["## Import the libraries\n","\n","First, to make sure that all required libraries are available, run the cell below.\n"]},{"cell_type":"code","id":"f4a057e4-06fb-4579-99c5-11a3ab4b6b8d","metadata":{},"outputs":[],"source":["!pip install numpy==2.2.0\n!pip install pandas==2.2.3\n!pip install scikit-learn==1.6.0\n!pip install matplotlib==3.9.3\n!pip install seaborn==0.13.2"]},{"cell_type":"markdown","id":"af6943b8-4c81-450d-a4c2-a307e5d6a53b","metadata":{},"outputs":[],"source":["Now, let's load required libraries.\n"]},{"cell_type":"code","id":"de2e6538-f440-4a7b-93de-0b4314d5784c","metadata":{},"outputs":[],"source":["import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import accuracy_score\n%matplotlib inline"]},{"cell_type":"markdown","id":"3e3929da-2171-421d-b2e9-586ff51cf2ee","metadata":{},"outputs":[],"source":["<div id=\"about_dataset\">\n","    <h2>About the data set</h2>\n","</div>\n"]},{"cell_type":"markdown","id":"9fddc7f9-5e1f-4099-92f6-edaa1a4c9733","metadata":{},"outputs":[],"source":["Imagine a telecommunications provider has segmented its customer base by service usage patterns, categorizing the customers into four groups. If demographic data can be used to predict group membership, the company can customize offers for individual prospective customers. It is a classification problem. That is, given the dataset,  with predefined labels, we need to build a model to be used to predict class of a new or unknown case.\n","\n","The example focuses on using demographic data, such as region, age, and marital, to predict usage patterns.\n","\n","The target field, called **custcat**, has four possible service categories that correspond to the four customer groups, as follows:\n","\n","1. Basic Service\n","2. E-Service\n","3. Plus Service\n","4. Total Service\n","\n","Our objective is to build a classifier to predict the service category for unknown cases. We will use a specific type of classification called K-nearest neighbors.\n"]},{"cell_type":"markdown","id":"dc287b0b-536e-4ad1-9176-1cfd4e38974c","metadata":{},"outputs":[],"source":["### Load Data \n"]},{"cell_type":"markdown","id":"4a7181e0-52d4-46c5-ae8b-db5e5a05a44d","metadata":{},"outputs":[],"source":["Let's read the data using pandas library and print the first five rows.\n"]},{"cell_type":"code","id":"78583062-da86-4402-934a-bc9e1556233b","metadata":{},"outputs":[],"source":["df = pd.read_csv('https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-ML0101EN-SkillsNetwork/labs/Module%203/data/teleCust1000t.csv')\ndf.head()"]},{"cell_type":"markdown","id":"33e811a2-0cb1-43e1-aa50-ee9aaeb371f8","metadata":{},"outputs":[],"source":["<div id=\"visualization_analysis\">\n","    <h2>Data Visualization and Analysis</h2> \n","</div>\n"]},{"cell_type":"markdown","id":"53c93aa1-3a8e-466b-80bd-91b17b4d14a1","metadata":{},"outputs":[],"source":["Let us first look at the class-wise distribution of the data set.\n"]},{"cell_type":"code","id":"a646057e-d3e5-4b9a-a113-52cdbb409720","metadata":{},"outputs":[],"source":["df['custcat'].value_counts()"]},{"cell_type":"markdown","id":"6f9c7a7f-ce18-4f25-8499-3939786e6934","metadata":{},"outputs":[],"source":["Hence, we can say that we have records of 281 customers who opt for Plus Services, 266 for Basic-services, 236 for Total Services, and 217 for E-Services. It can thus be seen that the data set is mostly balanced between the different classes and requires no special means of accounting for class bias.\n"]},{"cell_type":"markdown","id":"391841bb-9635-4cc3-9655-fe9f8246bb80","metadata":{},"outputs":[],"source":["We can also visualize the correlation map of the data set to determine how the different features are related to each other. \n"]},{"cell_type":"code","id":"af602717-0e81-479a-8448-581ad09fc6c9","metadata":{},"outputs":[],"source":["correlation_matrix = df.corr()\n\nplt.figure(figsize=(10, 8))\nsns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5)"]},{"cell_type":"markdown","id":"83707371-59d0-4e59-acb0-e7f7b9b3f70b","metadata":{},"outputs":[],"source":["As is visible from the correlation map, some features have beeter correlation among them than others, basically indicating the depth of relationship between the two features. What is of interest to us is the correlation of the target feature, i.e. `custcat` with all the other features. This will help us identify which features should be focussed on for modeling and which ones can be ignored. \n","\n","The following code snippet will give us a list of features sorted in the descending order of their absolute correlation values with respect to the target field.\n"]},{"cell_type":"code","id":"689fd288-0d66-4f35-b77e-daf37d86822d","metadata":{},"outputs":[],"source":["correlation_values = abs(df.corr()['custcat'].drop('custcat')).sort_values(ascending=False)\ncorrelation_values"]},{"cell_type":"markdown","id":"29814976-0005-43ce-8c1b-22dd12ca9630","metadata":{},"outputs":[],"source":["This shows us that the features `retire` and `gender` have the least effect on `custcat` while `ed` and `tenure` have the most effect.\n"]},{"cell_type":"markdown","id":"122be4ad-b107-43ad-a057-faca0aab5573","metadata":{},"outputs":[],"source":["### Separate the input and target features\n"]},{"cell_type":"markdown","id":"87b52b29-0212-4501-b00a-189d1ad688a1","metadata":{},"outputs":[],"source":["Now, we can separate the data into the input data set and the target data set.\n"]},{"cell_type":"code","id":"8f71f720-f0f0-403d-a207-fc01a4b9a02f","metadata":{},"outputs":[],"source":["X = df.drop('custcat',axis=1)\ny = df['custcat']"]},{"cell_type":"markdown","id":"8f06f9a0-afe8-45a5-80fa-166b319ef329","metadata":{},"outputs":[],"source":["## Normalize Data\n"]},{"cell_type":"markdown","id":"001ceebd-8b19-44f8-b290-ce2d2694fe7a","metadata":{},"outputs":[],"source":["Data normalization is important for the KNN model. \n","\n","KNN makes predictions based on the distance between data points (samples), i.e. for a given test point, the algorithm finds the k-nearest neighbors by measuring the distance between the test point and other data points in the dataset. By normalizing / standardizing the data, you ensure that all features contribute equally to the distance calculation. Since normalization scales each feature to have zero mean and unit variance, it puts all features on the same scale (with no feature dominating due to its larger range).\n","\n","This helps KNN make better decisions based on the actual relationships between features, not just on the magnitude of their values.\n"]},{"cell_type":"code","id":"885c9a30-0b63-447d-bf44-f3db637d9c5b","metadata":{},"outputs":[],"source":["X_norm = StandardScaler().fit_transform(X)"]},{"cell_type":"markdown","id":"7f479a22-b8b4-4eb0-a36b-0362b12f91cb","metadata":{},"outputs":[],"source":["### Train Test Split\n","\n","Now, you should separate the training and the testing data. You can retain 20% of the data for testing purposes and use the rest for training. Assigning a random state ensures reproducibility of the results across multiple executions.\n"]},{"cell_type":"code","id":"1309eb96-fa1b-46de-9003-075762d35f2b","metadata":{},"outputs":[],"source":["X_train, X_test, y_train, y_test = train_test_split(X_norm, y, test_size=0.2, random_state=4)"]},{"cell_type":"markdown","id":"b6d8b576-837a-4024-86cf-c07747749cf4","metadata":{},"outputs":[],"source":["## KNN Classification\n"]},{"cell_type":"markdown","id":"95fa628e-4860-4e91-9f1e-88e2b2a49196","metadata":{},"outputs":[],"source":["Once the data is in place, we can now execute the training of the model.\n"]},{"cell_type":"markdown","id":"cbb4614d-e5d6-4853-bb15-a6480af08a57","metadata":{},"outputs":[],"source":["### Training\n","Initially, you may start by using a small value as the value of k, say k = 4.\n"]},{"cell_type":"code","id":"896d8c5f-3991-4336-824d-6d35df35b280","metadata":{},"outputs":[],"source":["k = 3\n#Train Model and Predict  \nknn_classifier = KNeighborsClassifier(n_neighbors=k)\nknn_model = knn_classifier.fit(X_train,y_train)"]},{"cell_type":"markdown","id":"d78c2406-6e33-44f6-b328-80d9706add71","metadata":{},"outputs":[],"source":["### Predicting\n","Once the model is trained, we can now use this model to generate predictions for the test set. \n"]},{"cell_type":"code","id":"70c58503-b36b-46a0-8755-47b017b107c7","metadata":{},"outputs":[],"source":["yhat = knn_model.predict(X_test)"]},{"cell_type":"markdown","id":"fd0d367f-76cd-4f39-b672-b3d11587897d","metadata":{},"outputs":[],"source":["### Accuracy evaluation\n","\n","In multilabel classification, **accuracy classification score** is a function that computes subset accuracy. This function is equal to the jaccard_score function. Essentially, it calculates how closely the actual labels and predicted labels are matched in the test set.\n"]},{"cell_type":"code","id":"ba3e2a6c-d722-47b3-9aa4-e74b133c694f","metadata":{},"outputs":[],"source":["print(\"Test set Accuracy: \", accuracy_score(y_test, yhat))"]},{"cell_type":"markdown","id":"198f5125-3dd2-4ef9-84b2-b4d87f4d30d4","metadata":{},"outputs":[],"source":["### Exercise 1\n","Can you build the model again, but this time with k=6?\n"]},{"cell_type":"code","id":"0778bafa-c152-4eca-ae78-d6edc4aed804","metadata":{},"outputs":[],"source":["# write your code here\n"]},{"cell_type":"markdown","id":"46a9e94d-b0ea-46ed-a32c-6b1bc3094e91","metadata":{},"outputs":[],"source":["<details><summary>Click here for the solution</summary>\n","\n","```python\n","k = 6\n","knn_model_6 = KNeighborsClassifier(n_neighbors = k).fit(X_train,y_train)\n","yhat6 = knn_model_6.predict(X_test)\n","print(\"Test set Accuracy: \", accuracy_score(y_test, yhat6))\n","\n","```\n","\n","</details>\n"]},{"cell_type":"markdown","id":"f846e5fa-8b2b-42da-a0f7-40b90756473f","metadata":{},"outputs":[],"source":["### Choosing the correct value of k\n","\n","K in KNN, is the number of nearest neighbors to examine. However, the choice of the value of 'k' clearly affects the model. Therefore, the appropriate choice of the value of the variable `k` becomes an important task. The general way of doing this is to train the model on a set of different values of k and noting the performance of the trained model on the testing set. The model with the best value of `accuracy_score` is the one with the ideal value of the parameter k.\n","\n","Check the performance of the model for 10 values of k, ranging from 1-9. You can evaluate the accuracy along with the standard deviation of the accuracy as well to get a holistic picture of the model performance.\n"]},{"cell_type":"code","id":"0ad89540-189f-4ecf-be68-558d67b7206d","metadata":{},"outputs":[],"source":["Ks = 10\nacc = np.zeros((Ks))\nstd_acc = np.zeros((Ks))\nfor n in range(1,Ks+1):\n    #Train Model and Predict  \n    knn_model_n = KNeighborsClassifier(n_neighbors = n).fit(X_train,y_train)\n    yhat = knn_model_n.predict(X_test)\n    acc[n-1] = accuracy_score(y_test, yhat)\n    std_acc[n-1] = np.std(yhat==y_test)/np.sqrt(yhat.shape[0])"]},{"cell_type":"markdown","id":"a2afd14b-ba10-4c8a-8201-5dde8a108e01","metadata":{},"outputs":[],"source":["### Plot the model accuracy for a different number of neighbors.\n","Now, you can plot the model accuracy and the standard deviation to identify the model with the most suited value of k.\n"]},{"cell_type":"code","id":"4d13e937-0b64-4d79-8981-bf6fe6f3e775","metadata":{},"outputs":[],"source":["plt.plot(range(1,Ks+1),acc,'g')\nplt.fill_between(range(1,Ks+1),acc - 1 * std_acc,acc + 1 * std_acc, alpha=0.10)\nplt.legend(('Accuracy value', 'Standard Deviation'))\nplt.ylabel('Model Accuracy')\nplt.xlabel('Number of Neighbors (K)')\nplt.tight_layout()\nplt.show()"]},{"cell_type":"code","id":"52bed615-796e-4002-9826-236a121c41ce","metadata":{},"outputs":[],"source":["print( \"The best accuracy was with\", acc.max(), \"with k =\", acc.argmax()+1) "]},{"cell_type":"markdown","id":"9c4093f4-889d-4445-be2a-d0320627cc0c","metadata":{},"outputs":[],"source":["However, since this graph is still rising, there can be a chance that the model will give a better performance with an even higher value of k.\n","\n","### Exercise 2\n","Run the training model for 30 values of k and then again for 100 values of k. Identify the value of k that best suits this data and the accuracy on the test set for this model.\n"]},{"cell_type":"markdown","id":"1f5e6ef4-6b66-4f48-b3a8-105d465c14c6","metadata":{},"outputs":[],"source":["Enter you answer here\n"]},{"cell_type":"markdown","id":"dc47f7fc-2cdc-4f52-b760-0ad4c002323c","metadata":{},"outputs":[],"source":["<details><summary>Click here for answer</summary>\n","Execute the cells above by changing the value of the variable `Ks` to 30 and then try again by changing it to 100. In case of 30 values, you should find that the best value of accuracy is achieved for k=30, which again indicates that there is a further scope of improvement. In case of 100 values, you will find that the best value of accuracy is achieved for k=38, after which the model performance starts declining. Hence, the best choice of the value for k is 38, yielding 41% accuracy score.\n","</details>\n"]},{"cell_type":"markdown","id":"e1b84d83-84e8-45ab-8592-6e4561f1df2b","metadata":{},"outputs":[],"source":["### Exercise 3\n","Plot the variation of the accuracy score for the **training set** for 100 value of Ks.\n"]},{"cell_type":"code","id":"eed28edb-dafc-4176-9d5e-5b84785ba043","metadata":{},"outputs":[],"source":["# your code here\n"]},{"cell_type":"markdown","id":"18fbfa07-d82b-46a4-b2db-e887d35c908f","metadata":{},"outputs":[],"source":["<details><summary>Click here for the solution</summary>\n","\n","```python\n","Ks =100\n","acc = np.zeros((Ks-1))\n","std_acc = np.zeros((Ks-1))\n","for n in range(1,Ks):\n","    #Train Model and Predict  \n","    knn_model_n = KNeighborsClassifier(n_neighbors = n).fit(X_train,y_train)\n","    yhat = knn_model_n.predict(X_train)\n","    acc[n-1] = accuracy_score(y_train, yhat)\n","    std_acc[n-1] = np.std(yhat==y_train)/np.sqrt(yhat.shape[0])\n","\n","plt.plot(range(1,Ks),acc,'g')\n","plt.fill_between(range(1,Ks),acc - 1 * std_acc, acc + 1 * std_acc, alpha=0.10)\n","plt.legend(('Accuracy value', 'Standard Deviation'))\n","plt.ylabel('Model Accuracy')\n","plt.xlabel('Number of Neighbors (K)')\n","plt.tight_layout()\n","plt.show()\n","```\n","</details>\n"]},{"cell_type":"markdown","id":"5a87d037-09df-4c48-b0ff-c2899d1d7ca2","metadata":{},"outputs":[],"source":["### Exercise 4\n","\n","Can you justify why the model performance on training data is deteriorating with increase in the value of k?\n"]},{"cell_type":"markdown","id":"348c7b60-5594-4ddc-b057-ee82b4824654","metadata":{},"outputs":[],"source":["Enter you answer here\n"]},{"cell_type":"markdown","id":"ee765fb0-2eb4-4694-afd6-dfd54bc4f7fc","metadata":{},"outputs":[],"source":["<details><summary>Click here for the solution</summary>\n","\n","When k is small (e.g., k=1), the model is highly sensitive to the individual points in the dataset. The prediction for each point is based on its closest neighbor, which can lead to highly specific and flexible boundaries. This leads to overfitting on the training data, meaning the model will perform very well on the training set, potentially achieving 100% accuracy. However, it may generalize poorly to unseen data.\n","When k is large, the model starts to take into account more neighbors when making predictions. This has two main consequences:\n","1. Smoothing of the Decision Boundary: The decision boundary becomes smoother, which means the model is less sensitive to the noise or fluctuations in the training data. \n","2. Less Specific Predictions: With a larger k, the model considers more neighbors and therefore makes more generalized predictions, which can lead to fewer instances being classified perfectly.\n","\n","As a result, the model starts to become less flexible, and its ability to memorize the training data (which can lead to perfect accuracy with small k) is reduced.\n","</details>\n"]},{"cell_type":"markdown","id":"a8d09fd3-e49e-4e4b-a0d8-299897563863","metadata":{},"outputs":[],"source":["### Exercise 5\n","We can see that even the with the optimum values, the KNN model is not performing that well on the given data set. Can you think of the possible reasons for this?\n"]},{"cell_type":"markdown","id":"da67efe5-83d1-4e63-b2e7-4d5c861b49cd","metadata":{},"outputs":[],"source":["Enter you answer here\n"]},{"cell_type":"markdown","id":"89ee083f-da19-4f59-b2db-f85f219a3221","metadata":{},"outputs":[],"source":["<details><summary>Click here for the solution</summary>\n","The weak performance on the model can be due to multiple reasons.\n","    1. The KNN model relies entirely on the raw feature space at inference time. If the features do no provide clear boundaries between classes, KNN model cannot compensate through optimization or feature transformation.\n","    2. For a high number of weakly correlated features, the number of dimensions increases, the distance between points tend to become more uniform, reducing the discriminative power of KNN.\n","    3. The algorithm treats all features equally when computing distances. Hence, weakly correalted features can introduce noise or irrelevant variations in the feature space making it harder for KNN to find meaningful neighbours.\n","</details>\n"]},{"cell_type":"markdown","id":"3fa0ba74-111c-4a8e-bd7c-2acdeb96af76","metadata":{},"outputs":[],"source":["### Congratulations! You're ready to move on to your next lesson!\n","\n","## Author\n","<a href=\"https://www.linkedin.com/in/abhishek-gagneja-23051987/\" target=\"_blank\">Abhishek Gagneja</a>\n","### Other Contributors\n","<a href=\"https://www.linkedin.com/in/jpgrossman/\" target=\"_blank\">Jeff Grossman</a>  \n"," \n","<h3 align=\"center\"> © IBM Corporation. All rights reserved. <h3/>\n","\n"," \n","<!--\n","## Change Log\n"," \n"," \n","|  Date (YYYY-MM-DD) |  Version       | Changed By     | Change Description                  |\n","|---|---|---|---|\n","| 2024-11-29         | 3.1            | Jeff Grossman  | Review and make minor edits         |\n","| 2024-10-31         | 3.0            | Abhishek Gagneja  | Rewrite                             |\n","| 2020-11-03         | 2.1            | Lakshmi        | Made changes in URL                 |\n","| 2020-11-03         | 2.1            | Lakshmi        | Made changes in URL                 |\n","| 2020-08-27         | 2.0            | Lavanya        | Moved lab to course repo in GitLab  |\n","|   |   |   |   |\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.9"},"prev_pub_hash":"4dd4704a915b4400fa7232b7fb288dc931b2f24a762db49896d9e9beb539fd0e"},"nbformat":4,"nbformat_minor":4}